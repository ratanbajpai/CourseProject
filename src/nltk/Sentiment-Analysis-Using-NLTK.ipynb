{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS 410 Course Project\n",
    "# Sentiment Analysis of twitter samples using NLTK\n",
    "# This will use the downloaded \"twitter_samples\" corpus of NLTK. The twitter samples contain labeled positive and\n",
    "# negative tweets, which will be used to train a model. Once the model is trained, the sentiment analysis task will\n",
    "# be performed on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK, twitter_samples and other imports\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "import re, string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to remove puntuation, urls and twitter handles from the tokenized tweet text\n",
    "# After the tweet tokens are cleaned, they are lemmatized and the lemmatized list is returned\n",
    "def clean_and_lemmatize_tokens(tweet_tokens, word_lemmatizer, stop_words = ()):\n",
    "    lemmatized_tokens = []\n",
    "    # Use pos_tag function to attach part of speech to the tokens.\n",
    "    # For each token and POS tag...\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # Lowercase all the letters\n",
    "        token = token.lower()\n",
    "        # Remove twitter handles\n",
    "        token = re.sub(\"@[A-Za-z0-9_]+\",\"\", token)\n",
    "        # Remove hashtags\n",
    "        token = re.sub(\"#[A-Za-z0-9_]+\",\"\", token)\n",
    "        # Remove urls\n",
    "        token = re.sub(r\"http\\S+\", \"\", token)\n",
    "        token = re.sub(r\"www.\\S+\", \"\", token)\n",
    "        # Remove puntuations\n",
    "        token = re.sub('[()!?]', \"\", token)\n",
    "        token = re.sub('\\[.*?\\]', \"\", token)\n",
    "        # Remove alpha-numeric characters\n",
    "        token = re.sub(\"[^a-z0-9]\", \"\", token)\n",
    "        # Remove stop words\n",
    "        if len(token) > 0 and token not in stop_words:\n",
    "            # Lemmatize the cleaned token\n",
    "            if tag.startswith(\"NN\"):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            # Use the passed in word lemmatizer\n",
    "            token = word_lemmatizer.lemmatize(token, pos)\n",
    "            # Append to the lemmatized token list\n",
    "            lemmatized_tokens.append(token)\n",
    "    return lemmatized_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a list of cleaned and lemmatized tokens and adds \"True\" for each\n",
    "# feature to be included in the feature list. It also labels the data as \"Positive\" or\n",
    "# \"Negative\"\n",
    "def prep_model_data(lemmatized_tokens_list, label):\n",
    "    # Initialize a list of training or testing data. This will be a list\n",
    "    # of tuples where first element is a dictionary containing featues and\n",
    "    # second element is the label, whether the tweet sentiment is \"Positive\" or\n",
    "    # \"Negative\". Each element of the list will look as follows:\n",
    "    # ({feature1: True, feature2: True,....}, \"Positive\"). This is the format\n",
    "    # in which the classifier takes the input data.\n",
    "    classifier_input_data_list = []\n",
    "    for tweet_tokens in lemmatized_tokens_list:\n",
    "        # Initialize the dictionary that will have features and include flag\n",
    "        input_features = {}\n",
    "        for token in tweet_tokens:\n",
    "            # We will include all tokens as input_features and mark them True\n",
    "            input_features[token] = \"True\"\n",
    "        # Create a tuple of the above dictionary and the given label\n",
    "        input_features_with_label = (input_features, label)\n",
    "        # Add this input data to the list\n",
    "        classifier_input_data_list.append(input_features_with_label)\n",
    "    # Return the classifier input data list\n",
    "    return classifier_input_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables for positive, negative and neutral tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "hopeless for tmr :(\n"
     ]
    }
   ],
   "source": [
    "# Print tweets data\n",
    "print(positive_tweets[0])\n",
    "print(negative_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize positive and negative tweets\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['hopeless', 'for', 'tmr', ':(']\n"
     ]
    }
   ],
   "source": [
    "# Print tokenized tweet tokens\n",
    "print(positive_tweet_tokens[0])\n",
    "print(negative_tweet_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n",
      "[('hopeless', 'NN'), ('for', 'IN'), ('tmr', 'NN'), (':(', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Print tweet tokens with parts of speech attached\n",
    "print(pos_tag(positive_tweet_tokens[0]))\n",
    "print(pos_tag(negative_tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove links, twitter handles, hashtags, puntuations etc. and lemmatize\n",
    "# Get stopwords list\n",
    "stop_words = stopwords.words('english')\n",
    "# Get word lemmatizer\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "# Initialize output lists\n",
    "positive_lemmatized_tokens_list = []\n",
    "negative_lemmatized_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_lemmatized_tokens_list.append(clean_and_lemmatize_tokens(tokens, word_lemmatizer, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_lemmatized_tokens_list.append(clean_and_lemmatize_tokens(tokens, word_lemmatizer, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top', 'engage', 'member', 'community', 'week']\n",
      "['hopeless', 'tmr']\n"
     ]
    }
   ],
   "source": [
    "# Test the cleaned and lemmatized outputs\n",
    "print(positive_lemmatized_tokens_list[0])\n",
    "print(negative_lemmatized_tokens_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step creates a list of tuple where each tuple contains a dictionary of\n",
    "# input features and a label. We create training data for a set of positive\n",
    "# and negative tweets. Each dictionary contains features as key and a include\n",
    "# flag (True, False to include feature or not). The label indicates if the\n",
    "# tweet sentiment is positive or negative (labeled data).\n",
    "positive_labeled_dataset = prep_model_data(positive_lemmatized_tokens_list, \"Positive\")\n",
    "negative_labeled_dataset = prep_model_data(negative_lemmatized_tokens_list, \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'top': 'True', 'engage': 'True', 'member': 'True', 'community': 'True', 'week': 'True'}, 'Positive')\n",
      "({'hopeless': 'True', 'tmr': 'True'}, 'Negative')\n"
     ]
    }
   ],
   "source": [
    "# Test the input data set for model\n",
    "print(positive_labeled_dataset[0])\n",
    "print(negative_labeled_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the first 3750 tweets data from each set to total of 7500 items for training data\n",
    "training_dataset = positive_labeled_dataset[:3750] + negative_labeled_dataset[:3750]\n",
    "# Shuffle training data to remove any bias\n",
    "random.shuffle(training_dataset)\n",
    "# Use remaining 1250 items from each set as testing dataset\n",
    "testing_dataset = positive_labeled_dataset[3750:] + negative_labeled_dataset[3750:]\n",
    "# Shuffle testing data to remove any bias\n",
    "random.shuffle(testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'ate': 'True', 'menille': 'True', 'need': 'True', 'youuuu': 'True'}, 'Negative')\n",
      "({'fave': 'True', 'unfollows': 'True'}, 'Negative')\n"
     ]
    }
   ],
   "source": [
    "# Test the model training data\n",
    "print(training_dataset[0])\n",
    "print(testing_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is: 0.7368\n",
      "Most Informative Features\n",
      "                followed = 'True'         Negati : Positi =     34.3 : 1.0\n",
      "                follower = 'True'         Positi : Negati =     26.2 : 1.0\n",
      "                    glad = 'True'         Positi : Negati =     25.7 : 1.0\n",
      "                     x15 = 'True'         Negati : Positi =     23.7 : 1.0\n",
      "                  arrive = 'True'         Positi : Negati =     22.2 : 1.0\n",
      "                     sad = 'True'         Negati : Positi =     21.7 : 1.0\n",
      "                       p = 'True'         Positi : Negati =     21.4 : 1.0\n",
      "                    sick = 'True'         Negati : Positi =     19.7 : 1.0\n",
      "               community = 'True'         Positi : Negati =     16.3 : 1.0\n",
      "                  justin = 'True'         Negati : Positi =     15.0 : 1.0\n",
      "                     ugh = 'True'         Negati : Positi =     13.7 : 1.0\n",
      "                    miss = 'True'         Negati : Positi =     13.3 : 1.0\n",
      "                      aw = 'True'         Negati : Positi =     13.0 : 1.0\n",
      "              definitely = 'True'         Positi : Negati =     13.0 : 1.0\n",
      "                follback = 'True'         Positi : Negati =     12.3 : 1.0\n",
      "                   shame = 'True'         Negati : Positi =     12.3 : 1.0\n",
      "              bestfriend = 'True'         Positi : Negati =     11.0 : 1.0\n",
      "              appreciate = 'True'         Positi : Negati =     10.6 : 1.0\n",
      "                  ignore = 'True'         Negati : Positi =     10.3 : 1.0\n",
      "             opportunity = 'True'         Positi : Negati =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train the Naive Bayes Classifier\n",
    "nb_classifier = NaiveBayesClassifier.train(training_dataset)\n",
    "\n",
    "# Now test the trained model accuracy on testing dataset\n",
    "testing_accuracy = classify.accuracy(nb_classifier, testing_dataset)\n",
    "print(\"Testing accuracy is:\", testing_accuracy)\n",
    "\n",
    "# Top 20 most informative features\n",
    "nb_classifier.show_most_informative_features(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
